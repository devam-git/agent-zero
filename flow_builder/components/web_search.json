{
    "template": {
        "_type": "Component",
        "code": {
            "type": "code",
            "required": true,
            "placeholder": "",
            "list": false,
            "show": true,
            "multiline": true,
            "value": "import re\nfrom urllib.parse import parse_qs, unquote, urlparse\n\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\n\nfrom langflow.custom import Component\nfrom langflow.io import IntInput, MessageTextInput, Output\nfrom langflow.schema import DataFrame\nfrom langflow.services.deps import get_settings_service\n\n\nclass WebSearchComponent(Component):\n    display_name = \"Web Search\"\n    description = \"Performs a basic DuckDuckGo search (HTML scraping). May be subject to rate limits.\"\n    icon = \"search\"\n    name = \"WebSearchNoAPI\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"query\",\n            display_name=\"Search Query\",\n            info=\"Keywords to search for.\",\n            tool_mode=True,\n            required=True,\n        ),\n        IntInput(\n            name=\"timeout\",\n            display_name=\"Timeout\",\n            info=\"Timeout for the web search request.\",\n            value=5,\n            advanced=True,\n        ),\n    ]\n\n    outputs = [Output(name=\"results\", display_name=\"Search Results\", method=\"perform_search\")]\n\n    def validate_url(self, string: str) -> bool:\n        url_regex = re.compile(\n            r\"^(https?:\\/\\/)?\" r\"(www\\.)?\" r\"([a-zA-Z0-9.-]+)\" r\"(\\.[a-zA-Z]{2,})?\" r\"(:\\d+)?\" r\"(\\/[^\\s]*)?$\",\n            re.IGNORECASE,\n        )\n        return bool(url_regex.match(string))\n\n    def ensure_url(self, url: str) -> str:\n        if not url.startswith((\"http://\", \"https://\")):\n            url = \"https://\" + url\n        if not self.validate_url(url):\n            msg = f\"Invalid URL: {url}\"\n            raise ValueError(msg)\n        return url\n\n    def _sanitize_query(self, query: str) -> str:\n        \"\"\"Sanitize search query.\"\"\"\n        # Remove potentially dangerous characters\n        return re.sub(r'[<>\"\\']', \"\", query.strip())\n\n    def perform_search(self) -> DataFrame:\n        query = self._sanitize_query(self.query)\n        if not query:\n            msg = \"Empty search query\"\n            raise ValueError(msg)\n        headers = {\"User-Agent\": get_settings_service().settings.user_agent}\n        params = {\"q\": query, \"kl\": \"us-en\"}\n        url = \"https://html.duckduckgo.com/html/\"\n\n        try:\n            response = requests.get(url, params=params, headers=headers, timeout=self.timeout)\n            response.raise_for_status()\n        except requests.RequestException as e:\n            self.status = f\"Failed request: {e!s}\"\n            return DataFrame(pd.DataFrame([{\"title\": \"Error\", \"link\": \"\", \"snippet\": str(e), \"content\": \"\"}]))\n\n        if not response.text or \"text/html\" not in response.headers.get(\"content-type\", \"\").lower():\n            self.status = \"No results found\"\n            return DataFrame(\n                pd.DataFrame([{\"title\": \"Error\", \"link\": \"\", \"snippet\": \"No results found\", \"content\": \"\"}])\n            )\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        results = []\n\n        for result in soup.select(\"div.result\"):\n            title_tag = result.select_one(\"a.result__a\")\n            snippet_tag = result.select_one(\"a.result__snippet\")\n            if title_tag:\n                raw_link = title_tag.get(\"href\", \"\")\n                parsed = urlparse(raw_link)\n                uddg = parse_qs(parsed.query).get(\"uddg\", [\"\"])[0]\n                decoded_link = unquote(uddg) if uddg else raw_link\n\n                try:\n                    final_url = self.ensure_url(decoded_link)\n                    page = requests.get(final_url, headers=headers, timeout=self.timeout)\n                    page.raise_for_status()\n                    content = BeautifulSoup(page.text, \"lxml\").get_text(separator=\" \", strip=True)\n                except requests.RequestException as e:\n                    final_url = decoded_link\n                    content = f\"(Failed to fetch: {e!s}\"\n\n                results.append(\n                    {\n                        \"title\": title_tag.get_text(strip=True),\n                        \"link\": final_url,\n                        \"snippet\": snippet_tag.get_text(strip=True) if snippet_tag else \"\",\n                        \"content\": content,\n                    }\n                )\n\n        df_results = pd.DataFrame(results)\n        return DataFrame(df_results)\n",
            "fileTypes": [],
            "file_path": "",
            "password": false,
            "name": "code",
            "advanced": true,
            "dynamic": true,
            "info": "",
            "load_from_db": false,
            "title_case": false
        },
        "query": {
            "tool_mode": true,
            "trace_as_input": true,
            "trace_as_metadata": true,
            "load_from_db": false,
            "list": false,
            "list_add_label": "Add More",
            "required": true,
            "placeholder": "",
            "show": true,
            "name": "query",
            "value": "",
            "display_name": "Search Query",
            "advanced": false,
            "input_types": [
                "Message"
            ],
            "dynamic": false,
            "info": "Keywords to search for.",
            "title_case": false,
            "type": "str",
            "_input_type": "MessageTextInput"
        },
        "timeout": {
            "tool_mode": false,
            "trace_as_metadata": true,
            "list": false,
            "list_add_label": "Add More",
            "required": false,
            "placeholder": "",
            "show": true,
            "name": "timeout",
            "value": 5,
            "display_name": "Timeout",
            "advanced": true,
            "dynamic": false,
            "info": "Timeout for the web search request.",
            "title_case": false,
            "type": "int",
            "_input_type": "IntInput"
        }
    },
    "description": "Performs a basic DuckDuckGo search (HTML scraping). May be subject to rate limits.",
    "icon": "search",
    "base_classes": [
        "DataFrame"
    ],
    "display_name": "Web Search",
    "documentation": "",
    "minimized": false,
    "custom_fields": {},
    "output_types": [],
    "pinned": false,
    "conditional_paths": [],
    "frozen": false,
    "outputs": [
        {
            "types": [
                "DataFrame"
            ],
            "selected": "DataFrame",
            "name": "results",
            "hidden": null,
            "display_name": "Search Results",
            "method": "perform_search",
            "value": "__UNDEFINED__",
            "cache": true,
            "required_inputs": null,
            "allows_loop": false,
            "group_outputs": false,
            "options": null,
            "tool_mode": true
        }
    ],
    "field_order": [
        "query",
        "timeout"
    ],
    "beta": false,
    "legacy": false,
    "edited": false,
    "metadata": {},
    "tool_mode": false,
    "category": "data",
    "key": "WebSearchNoAPI",
    "score": 0.10272033187433852,
    "official": false
}